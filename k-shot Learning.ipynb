{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import itertools\n",
    "from random import sample\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import LSTM, Bidirectional, Embedding\n",
    "from keras import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Lambda, Subtract\n",
    "from keras import backend as K\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data = pd.read_csv('./cleaned_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5907, 2)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_cleaned_content</th>\n",
       "      <th>annotation.labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pdhl poros ke ini sebenarnya mau nelikung...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>maap bosmending ngomong realita aja soal k...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seandainya sekutu berpaling apa masih tetap...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>saya dukung pak prabowo capresnamun plihan</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mencari yang setia dan sepemikiran itu sulit...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               final_cleaned_content annotation.labels\n",
       "0       pdhl poros ke ini sebenarnya mau nelikung...               joy\n",
       "1      maap bosmending ngomong realita aja soal k...               joy\n",
       "2     seandainya sekutu berpaling apa masih tetap...              fear\n",
       "3        saya dukung pak prabowo capresnamun plihan                joy\n",
       "4    mencari yang setia dan sepemikiran itu sulit...             trust"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_cleaned_content    5907\n",
       "annotation.labels           9\n",
       "dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy             1576\n",
       "trust           1309\n",
       "anticipation     873\n",
       "anger            797\n",
       "disgust          448\n",
       "fear             423\n",
       "sadness          405\n",
       "surprise          70\n",
       "other              6\n",
       "Name: annotation.labels, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data['annotation.labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating hold-out set for final evaluation. For \"other\" class took 2 samples and for remaining classes took 10 samples from each class\n",
    "labels = tweets_data['annotation.labels'].unique()\n",
    "temp_df = []\n",
    "for label in labels:\n",
    "    if label == 'other':\n",
    "        temp_df.append(tweets_data[tweets_data['annotation.labels'] == label].sample(frac=1).iloc[:2])\n",
    "    else:\n",
    "        temp_df.append(tweets_data[tweets_data['annotation.labels'] == label].sample(frac=1).iloc[:10])\n",
    "\n",
    "\n",
    "test_set = pd.concat(temp_df).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train set, which is all the tweets which are not present in the test set\n",
    "train_set = tweets_data[~tweets_data['final_cleaned_content'].isin(test_set['final_cleaned_content'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_set['final_cleaned_content'], train_set['annotation.labels']\n",
    "x_test, y_test = test_set['final_cleaned_content'], test_set['annotation.labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoding\n",
    "le = LabelEncoder()\n",
    "\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy             10\n",
       "trust           10\n",
       "fear            10\n",
       "surprise        10\n",
       "anger           10\n",
       "anticipation    10\n",
       "sadness         10\n",
       "disgust         10\n",
       "other            2\n",
       "Name: annotation.labels, dtype: int64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set['annotation.labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5825, 2)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets_left</th>\n",
       "      <th>tweets_right</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3188</th>\n",
       "      <td>terus terang anda bilang agama di perjual</td>\n",
       "      <td>tambahin ahhh   lg demam akrobatik gaya menj...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541</th>\n",
       "      <td>nantikan  bersama menteri kelautan dan perikan...</td>\n",
       "      <td>kelihatan kepalanya saja sama jam dinding saja...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>terkadang ambisi terlampau besar melebihi ke...</td>\n",
       "      <td>dan saya mau tanya cash bck ribu itu berlaku a...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>lagi kampanye maka semua yang dilakukan  sala...</td>\n",
       "      <td>gue pernah ke malaysia hawa disana itu pana...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4619</th>\n",
       "      <td>hai cfc loversnnkamu pecinta kartun doraemonnp...</td>\n",
       "      <td>sayang jatah gratis ongkir udah abis</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            tweets_left  \\\n",
       "3188          terus terang anda bilang agama di perjual   \n",
       "2541  nantikan  bersama menteri kelautan dan perikan...   \n",
       "668     terkadang ambisi terlampau besar melebihi ke...   \n",
       "2224   lagi kampanye maka semua yang dilakukan  sala...   \n",
       "4619  hai cfc loversnnkamu pecinta kartun doraemonnp...   \n",
       "\n",
       "                                           tweets_right  target  \n",
       "3188    tambahin ahhh   lg demam akrobatik gaya menj...     1.0  \n",
       "2541  kelihatan kepalanya saja sama jam dinding saja...     1.0  \n",
       "668   dan saya mau tanya cash bck ribu itu berlaku a...     1.0  \n",
       "2224     gue pernah ke malaysia hawa disana itu pana...     0.0  \n",
       "4619               sayang jatah gratis ongkir udah abis     0.0  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating training data pairs for learning similarity metric for Siamese network\n",
    "tweets_left = []\n",
    "tweets_right = []\n",
    "target = []\n",
    "\n",
    "labels = train_set['annotation.labels'].unique()\n",
    "for label in labels:\n",
    "    # 1) create similar categories pairs, with a corresponding target of 1\n",
    "    similar_tweets = train_set[train_set['annotation.labels'] == label]['final_cleaned_content']\n",
    "    \n",
    "    # Pick 300 random pairs \n",
    "    group_pairs = list(itertools.combinations(similar_tweets, 2)) \n",
    "    positive_pairs = sample(group_pairs, 300) if len(group_pairs) >= 300 else group_pairs\n",
    "    tweets_left.extend([p[0] for p in positive_pairs])\n",
    "    tweets_right.extend([p[1] for p in positive_pairs])\n",
    "    target.extend([1.]*len(positive_pairs))\n",
    "    \n",
    "    # 2) create pairs of examples with tweets from different categories, with a target set to 0\n",
    "    other_tweets = train_set[train_set['annotation.labels'] != label]['final_cleaned_content']\n",
    "    for i in range(len(positive_pairs)):\n",
    "        tweets_left.append(np.random.choice(similar_tweets))\n",
    "        tweets_right.append(np.random.choice(other_tweets))\n",
    "        target.append(0.)\n",
    "\n",
    "dataset = pd.DataFrame({\n",
    "        'tweets_left': tweets_left,\n",
    "        'tweets_right': tweets_right,\n",
    "        'target': target\n",
    "    }).sample(frac=1)  # Shuffle dataset\n",
    "\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4812, 3)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading fasttext wv dictionary\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('./ft_wv.pickle', 'rb') as f_obj:\n",
    "    wv_dictionary = pickle.load(f_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to sequence\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(dataset['tweets_left'].tolist() + dataset['tweets_right'].tolist())\n",
    "word_seq_train_left = tokenizer.texts_to_sequences(dataset['tweets_left'].tolist())\n",
    "word_seq_train_right = tokenizer.texts_to_sequences(dataset['tweets_right'].tolist())\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "\n",
    "word_seq_train_left = sequence.pad_sequences(word_seq_train_left, maxlen=70)\n",
    "word_seq_train_right = sequence.pad_sequences(word_seq_train_right, maxlen=70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pairs = [word_seq_train_left, word_seq_train_right]\n",
    "y_pairs = dataset['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of null word embeddings: 1\n"
     ]
    }
   ],
   "source": [
    "#embedding matrix\n",
    "\n",
    "words_not_found = []\n",
    "nb_words = 10000\n",
    "embed_dim = 300\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = wv_dictionary.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "left_input (InputLayer)         (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "right_input (InputLayer)        (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_network (Sequential) (None, 64)           3085248     left_input[0][0]                 \n",
      "                                                                 right_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pair_representations_difference (None, 64)           0           sequential_network[1][0]         \n",
      "                                                                 sequential_network[2][0]         \n",
      "__________________________________________________________________________________________________\n",
      "masltsm_distance (Lambda)       (None, 1)            0           pair_representations_difference[0\n",
      "==================================================================================================\n",
      "Total params: 3,085,248\n",
      "Trainable params: 85,248\n",
      "Non-trainable params: 3,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def exponent_neg_manhattan_distance(arms_difference):\n",
    "    \"\"\" Compute the exponent of the opposite of the L1 norm of a vector, to get the left/right inputs\n",
    "    similarity from the inputs differences. This function is used to turned the unbounded\n",
    "    L1 distance to a similarity measure between 0 and 1\"\"\"\n",
    "    return K.exp(-K.sum(K.abs(arms_difference), axis=1, keepdims=True))\n",
    "\n",
    "def siamese_lstm(max_length, embedding_matrix):\n",
    "    \"\"\" Define, compile and return a siamese LSTM model \"\"\"\n",
    "    input_shape = (max_length,)\n",
    "    left_input = Input(input_shape, name='left_input')\n",
    "    right_input = Input(input_shape, name='right_input')\n",
    "\n",
    "    # Define a single sequential model for both arms.\n",
    "    # In this example I've chosen a simple bidirectional LSTM with no dropout\n",
    "    seq = Sequential(name='sequential_network')\n",
    "    seq.add(Embedding(nb_words, 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "    seq.add(Bidirectional(LSTM(32, dropout=0., recurrent_dropout=0.)))\n",
    "    \n",
    "    left_output = seq(left_input)\n",
    "    right_output = seq(right_input)\n",
    "\n",
    "    # Here we subtract the neuron values of the last layer from the left arm \n",
    "    # with the corresponding values from the right arm\n",
    "    subtracted = Subtract(name='pair_representations_difference')([left_output, right_output])\n",
    "    malstm_distance = Lambda(exponent_neg_manhattan_distance, \n",
    "                             name='masltsm_distance')(subtracted)\n",
    "\n",
    "    siamese_net = Model(inputs=[left_input, right_input], outputs=malstm_distance)\n",
    "    siamese_net.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "    return siamese_net\n",
    "\n",
    "siamese_lstm = siamese_lstm(70, embedding_matrix)\n",
    "\n",
    "# Print a summary of the model mainly to know the number of trainable parameters\n",
    "siamese_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3849 samples, validate on 963 samples\n",
      "Epoch 1/6\n",
      "3849/3849 [==============================] - 9s 2ms/step - loss: 0.7321 - accuracy: 0.4921 - val_loss: 0.7072 - val_accuracy: 0.5016\n",
      "Epoch 2/6\n",
      "3849/3849 [==============================] - 7s 2ms/step - loss: 0.6747 - accuracy: 0.5804 - val_loss: 0.7008 - val_accuracy: 0.5275\n",
      "Epoch 3/6\n",
      "3849/3849 [==============================] - 8s 2ms/step - loss: 0.6469 - accuracy: 0.6550 - val_loss: 0.6954 - val_accuracy: 0.5348\n",
      "Epoch 4/6\n",
      "3849/3849 [==============================] - 7s 2ms/step - loss: 0.6280 - accuracy: 0.6802 - val_loss: 0.6926 - val_accuracy: 0.5358\n",
      "Epoch 5/6\n",
      "3849/3849 [==============================] - 7s 2ms/step - loss: 0.6124 - accuracy: 0.7129 - val_loss: 0.6918 - val_accuracy: 0.5421\n",
      "Epoch 6/6\n",
      "3849/3849 [==============================] - 8s 2ms/step - loss: 0.5997 - accuracy: 0.7314 - val_loss: 0.6897 - val_accuracy: 0.5400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a430076d8>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_lstm.fit(x_pairs, y_pairs, validation_split=0.2, epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_references = tokenizer.texts_to_sequences(x_train.tolist()) # Preprocess the training set examples\n",
    "x_references = sequence.pad_sequences(x_references, maxlen=70)\n",
    "\n",
    "def get_prediction(tweet):\n",
    "    \"\"\" Get the predicted tweet class, and the most similar tweet\n",
    "    in the train set. \"\"\"\n",
    "    x = tokenizer.texts_to_sequences([tweet])\n",
    "    x = sequence.pad_sequences(x, maxlen=70)\n",
    "    # Compute similarities of the tweet with all tweets in the train set\n",
    "    similarities = siamese_lstm.predict([[x[0]]*len(x_references), x_references])\n",
    "    most_similar_index = np.argmax(similarities)\n",
    "    \n",
    "    # The predicted category is the one of the most similar example from the train set\n",
    "    prediction = train_set['annotation.labels'].iloc[most_similar_index]\n",
    "    most_similar_example = train_set['final_cleaned_content'].iloc[most_similar_index]\n",
    "    return prediction, most_similar_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled test tweet: smg lariss mnis trs bt hisanahfcndn suksess trss berjyaanend smgaa berkmbang biakk sluruh otlett dari sambang sampaii merooukee \n",
      "True Label: joy\n",
      "Label prediction: joy\n",
      "Most similar example in train set:  assalamualaikum bapakkbrny gmn semoga bapak ganjar sekeluarga diberikan kesehatanbanyak rejekita\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 4\n",
    "pred, most_sim = get_prediction(x_test[sample_idx])\n",
    "\n",
    "print(f'Sampled test tweet: {x_test[sample_idx]}')\n",
    "print(f'True Label: {test_set[\"annotation.labels\"].iloc[sample_idx]}')\n",
    "print(f'Label prediction: {pred}')\n",
    "print(f'Most similar example in train set: {most_sim}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dac3ce3c4404791bdbd5ae85128b79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=82), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy (siamese model): 21.95 %\n"
     ]
    }
   ],
   "source": [
    "# Predicting test accuracy\n",
    "\n",
    "y_pred = [get_prediction(tweet)[0] for tweet in tqdm_notebook(test_set['final_cleaned_content'])]\n",
    "accuracy = accuracy_score(le.transform(y_pred), y_test)\n",
    "\n",
    "print(f'Test accuracy (siamese model): {100*accuracy:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing f1 score of the hold out set\n",
    "f1_score = f1_score(le.transform(y_pred), y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1724190818927661"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.20      0.18      0.19        11\n",
      "anticipation       0.40      0.27      0.32        15\n",
      "     disgust       0.20      0.40      0.27         5\n",
      "        fear       0.00      0.00      0.00         4\n",
      "         joy       0.50      0.25      0.33        20\n",
      "       other       0.00      0.00      0.00         1\n",
      "     sadness       0.20      0.22      0.21         9\n",
      "    surprise       0.00      0.00      0.00         1\n",
      "       trust       0.30      0.19      0.23        16\n",
      "\n",
      "   micro avg       0.22      0.22      0.22        82\n",
      "   macro avg       0.20      0.17      0.17        82\n",
      "weighted avg       0.31      0.22      0.25        82\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "print(classification_report(y_pred, le.inverse_transform(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
